import os
import re
import json
import numpy as np
from sklearn.neighbors import NearestNeighbors
from tqdm import tqdm
from collections import defaultdict
from openai import AzureOpenAI
from dotenv import load_dotenv
import ast # for use with the function get_retrieval_queries

load_dotenv()  # Load .env at the top

def sliding_window_chunk(text, window_size=2000, step_size=1000):
    """
    Splits text into overlapping chunks using a sliding window.
    Each chunk is window_size characters, sliding by step_size.
    Adds 'start' (global offset), 'chunk_id', and assigns correct 'page' using tag_chunks_with_page.
    """
    chunks = []
    n = len(text)
    start = 0
    chunk_num = 1
    while start < n:
        chunk_text = text[start:start+window_size]
        if chunk_text.strip():
            chunks.append({
                "chunk_id": f"chunk-{chunk_num}",
                "section": f"Window {chunk_num}",
                "text": chunk_text.strip(),
                "start": start  # global offset in chart text
                # 'page' will be assigned below
            })
        if start + window_size >= n:
            break
        start += step_size
        chunk_num += 1
    # Assign correct page numbers to each chunk
    chunks = tag_chunks_with_page(chunks, text)
    return chunks

def get_retrieval_queries(client, deploy_chat, user_question):
    """
    Use GPT to generate multiple focused retrieval queries from a user question.
    """
    prompt = f"""Rewrite the following clinical question as 3-5 short, keyword-focused queries for retrieving relevant medical record text. Focus on key terms, medications, diagnoses, or concepts.

Output ONLY a valid Python list of strings. Do not include any explanation or extra text.

Question: {user_question}
Queries:"""
    response = client.chat.completions.create(
        model=deploy_chat,
        messages=[
            {"role": "system", "content": "You are a clinical information retrieval assistant."},
            {"role": "user", "content": prompt}
        ],
        temperature=0.2
    )
    print("[DEBUG] Raw model output for queries:", response.choices[0].message.content.strip())
    # Expecting output like: ["midodrine indication", "midodrine orthostatic hypotension", ...]
    raw = response.choices[0].message.content.strip()
    # Remove code block markers if present
    if raw.startswith("```"):
        raw = raw.split("```")[-2].strip() if "```" in raw else raw
    try:
        queries = ast.literal_eval(raw)
        if isinstance(queries, list):
            return queries
    except Exception as e:
        print(f"[ERROR] Could not parse AI queries: {e}")
    # Fallback: just use the original question
    return [user_question]

def clean_and_split_chunks(chunks, max_length=8000):
    """
    Remove empty chunks and split any chunk longer than max_length into smaller pieces.
    """
    cleaned = []
    for chunk in chunks:
        text = chunk["text"].strip()
        if not text:
            continue
        if len(text) <= max_length:
            cleaned.append(chunk)
        else:
            # Split long chunk into smaller subchunks
            for i in range(0, len(text), max_length):
                subtext = text[i:i+max_length]
                cleaned.append({
                    "section": chunk["section"] + f" (part {i//max_length+1})",
                    "text": subtext,
                    "page": chunk["page"]
                })
    return cleaned

def clean_chunks_remove_duplicates_and_boilerplate(chunks, boilerplate_patterns=None):
    """
    Remove duplicate and boilerplate chunks.
    """
    if boilerplate_patterns is None:
        boilerplate_patterns = [
            r"electronically signed by",
            r"dictated by",
            r"this note was generated by",
            r"page \d+ of \d+",
            r"^date[:\s]",  # lines starting with 'Date:'
            r"^time[:\s]",  # lines starting with 'Time:'
            r"^signed[:\s]",  # lines starting with 'Signed:'
            r"for official use only \(fouo\)",
            r"disclaimer:.*the information contained in this document may contain privileged and confidential information including patient information protected by federal and state privacy laws\.",
            # Add more as needed
        ]
    seen = set()
    cleaned = []
    for chunk in chunks:
        text = chunk["text"].strip().lower()
        if not text or text in seen:
            continue
        if any(re.search(pat, text) for pat in boilerplate_patterns):
            continue
        cleaned.append(chunk)
        seen.add(text)
    return cleaned

def remove_boilerplate_phrases(text, boilerplate_patterns=None):
    """
    Remove boilerplate phrases from within a text chunk.
    """
    if boilerplate_patterns is None:
        boilerplate_patterns = [
            r"for official use only \(fouo\)",
            r'disclaimer:.*the information contained in this document may contain privileged and confidential information including patient information protected by federal and state privacy laws\.',
            r"electronically signed by",
            r"dictated by",
            r"this note was generated by",
            r"page \d+ of \d+",
            r"^date[:\s].*?$",  # lines starting with 'Date:'
            r"^time[:\s].*?$",  # lines starting with 'Time:'
            r"^signed[:\s].*?$",  # lines starting with 'Signed:'
            # Add more as needed
        ]
    cleaned = text
    for pat in boilerplate_patterns:
        cleaned = re.sub(pat, '', cleaned, flags=re.IGNORECASE | re.MULTILINE)
    return cleaned.strip()

def get_embeddings_batched(client, deploy_embed, texts, batch_size=300):
    embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding batches"):
        batch = texts[i:i + batch_size]
        try:
            response = client.embeddings.create(
                input=batch,
                model=deploy_embed
            )
            embeddings.extend([np.array(d.embedding) for d in response.data])
        except Exception as e:
            print(f"[ERROR] Embedding batch {i // batch_size + 1} failed: {e}")
    return np.array(embeddings)

def build_inverted_index(chunks):
    index = defaultdict(set)
    for i, chunk in enumerate(chunks):
        words = set(re.findall(r'\b\w+\b', chunk['text'].lower()))
        for word in words:
            index[word].add(i)
    return index

def hybrid_search(client, deploy_embed, query, chunks, vectors, inverted_index, top_k=20, semantic_ratio=0.75):
    # Keyword search (with stopwords filtered)
    STOPWORDS = set([
        "find", "patient", "how", "where", "when", "why", "who", "what", "which", "the", "and", "for", "or", "any", "of", "to", "in", "on", "with", "a", "an", "by", "at", "as", "is", "are", "was", "were", "be", "has", "have", "had", "that", "this", "these", "those", "from", "it", "but", "not"
    ])
    words = set(re.findall(r'\b\w+\b', query.lower())) - STOPWORDS
    keyword_hits = set()
    for word in words:
        keyword_hits.update(inverted_index.get(word, set()))
    keyword_chunks = [chunks[i] for i in keyword_hits]

    # Semantic search
    query_vec = get_embeddings_batched(client, deploy_embed, [query])[0]
    index = NearestNeighbors(n_neighbors=min(top_k, len(vectors)), metric="cosine")
    index.fit(vectors)
    _, indices = index.kneighbors([query_vec])
    semantic_chunks = [chunks[i] for i in indices[0]]

    # Combine: more semantic, less keyword
    n_semantic = int(top_k * semantic_ratio)
    n_keyword = top_k - n_semantic

    seen = set()
    results = []

    # Add semantic chunks first
    for c in semantic_chunks:
        key = (c['section'], c['page'], c['text'])
        if key not in seen:
            results.append(c)
            seen.add(key)
        if len(results) >= n_semantic:
            break

    # Add keyword chunks, but only up to n_keyword
    count_keyword = 0
    for c in keyword_chunks:
        key = (c['section'], c['page'], c['text'])
        if key not in seen:
            results.append(c)
            seen.add(key)
            count_keyword += 1
        if count_keyword >= n_keyword or len(results) >= top_k:
            break

    return results

def ask_gpt(client, deploy_chat, top_chunks, query=None, qa_history=None):
    # Build conversation context with explicit page for each chunk
    history_str = ""
    if qa_history:
        for i, qa in enumerate(qa_history):
            history_str += f"Previous Q{i+1}: {qa['question']}\nA{i+1}: {qa['answer']}\n"
    context = "\n\n".join([
        f"### Source: (Page {c.get('page','?')})\n{c['text']}"
        for c in top_chunks
    ])
    citation_instruction = (
        "\n\nIMPORTANT: For every fact or statement, include a parenthetical citation in the format (Page N) using the provided context. Never cite 'Window N' or offsets. Example: The patient's creatinine was 1.32 (Page 17). If you cannot find a citation, use (Unknown Page).\n"
    )
    if query:
        prompt = f"""You are a clinical assistant. Given the medical record segments below, answer the following question. Use the previous questions and answers for context if relevant.{citation_instruction}\nQuestion: \"{query}\"\n{history_str}{context}\n"""
    else:
        prompt = f"""You are a clinical assistant. Given the medical record segments below, write a concise narrative summary of the patient's active medical problems, treatments, and complications. Use the previous questions and answers for context if relevant.{citation_instruction}\n{history_str}{context}\n"""

    print("\n==== PROMPT SENT TO OPENAI ====\n")
    print(prompt)
    print("\n==== END PROMPT ====\n")

    try:
        response = client.chat.completions.create(
            model=deploy_chat,
            messages=[
                {"role": "system", "content": "You are a clinical reasoning assistant."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2
        )
        answer = response.choices[0].message.content
        # --- Post-process: replace any Window N, offsets, or ambiguous citations with (Page N) ---
        answer = postprocess_citations_page_only(answer, top_chunks)
        return answer
    except Exception as e:
        print(f"[ERROR] GPT call failed: {e}")
        return None

def postprocess_citations_page_only(answer, chunks):
    """
    Replace any (Window N), (Page N, ...), or ambiguous citations with (Page N) using chunk metadata.
    """
    # Build a mapping from chunk section to page
    section_map = {c['section']: c.get('page','?') for c in chunks}
    def repl(match):
        text = match.group(1)
        # Match Window N
        m = re.match(r"Window (\d+)", text)
        if m:
            section = f"Window {m.group(1)}"
            page = section_map.get(section, '?')
            return f"(Page {page})"
        # Match Page N (with or without offset)
        m2 = re.match(r"[Pp]age (\d+)(?:,\s*Offset \d+)?", text)
        if m2:
            page = m2.group(1)
            return f"(Page {page})"
        # Already in correct format
        m3 = re.match(r"[Pp]age (\d+)", text)
        if m3:
            return f"(Page {m3.group(1)})"
        return f"({text})"  # fallback
    # Replace all parenthetical citations
    return re.sub(r"\(([^)]+)\)", repl, answer)

def tag_chunks_with_page(chunks, text):
    """
    Assigns the correct page number to each chunk based on various page marker formats.
    Supports: 'Page X of Y', '## Page X', 'Page X', '- Page X -', and form feed '\f'.
    """
    page_regex = re.compile(
        r'(?:Page\s+(\d+)\s+of\s+\d+|##\s*Page\s+(\d+)|Page\s+(\d+)|-+\s*Page\s+(\d+)\s*-+|\f)',
        re.IGNORECASE
    )
    page_markers = []
    for m in page_regex.finditer(text):
        # Find the first non-None group (the page number)
        page_num = next((int(g) for g in m.groups() if g and g.isdigit()), None)
        page_markers.append((m.start(), page_num))
    page_markers.append((len(text), None))  # Sentinel

    for chunk in chunks:
        chunk_start = chunk["start"]
        for i, (pos, page_num) in enumerate(page_markers[:-1]):
            next_pos = page_markers[i+1][0]
            if pos <= chunk_start < next_pos:
                chunk["page"] = page_num if page_num is not None else 1
                chunk["page_offset"] = chunk_start - pos
                break
        else:
            chunk["page"] = 1
            chunk["page_offset"] = chunk_start
    return chunks

def tag_chunk_with_datetime(chunk):
    """
    Extracts date and time from chunk text if present and tags the chunk.
    """
    # Simple regex for common date and time formats
    date_match = re.search(r'(\d{1,2}/\d{1,2}/\d{2,4})', chunk["text"])
    time_match = re.search(r'(\d{1,2}:\d{2}(?:\s*[APMapm]{2})?)', chunk["text"])
    if date_match:
        chunk["date"] = date_match.group(1)
    if time_match:
        chunk["time"] = time_match.group(1)
    return chunk

def sentence_density_score(chunk):
    """
    Returns the number of sentences in the chunk as a proxy for narrative density.
    """
    # Count sentences using period, exclamation, or question mark as end
    sentences = re.split(r'[.!?]', chunk['text'])
    return sum(1 for s in sentences if s.strip())