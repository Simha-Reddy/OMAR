from __future__ import annotations
from typing import Any, Dict, List, Optional, Tuple
import re
import math
from datetime import datetime, timezone

import numpy as np

from app.ai_tools import embeddings as emb_api

# --- Core text utils and chunking ---

def _tokenize(text: str) -> List[str]:
    return re.findall(r"\b[\w'-]+\b", (text or '').lower())


def _bigrams(tokens: List[str]) -> List[str]:
    return [f"{tokens[i]}_{tokens[i+1]}" for i in range(len(tokens)-1)]


def tag_chunks_with_page(chunks: List[Dict[str, Any]], full_text: str) -> List[Dict[str, Any]]:
    """Assign page numbers to chunks based on simple page markers or form feed."""
    if not isinstance(full_text, str):
        full_text = str(full_text or '')
    page_regex = re.compile(
        r'(?:Page\s+(\d+)\s+of\s+\d+|##\s*Page\s+(\d+)|Page\s+(\d+)|-+\s*Page\s+(\d+)\s*-+|\f)',
        re.IGNORECASE,
    )
    page_markers: List[Tuple[int, Optional[int]]] = []
    for m in page_regex.finditer(full_text):
        page_num = next((int(g) for g in m.groups() if g and str(g).isdigit()), None)
        page_markers.append((m.start(), page_num))
    page_markers.append((len(full_text), None))
    if len(page_markers) <= 1:
        for ch in chunks:
            ch['page'] = ch.get('page', 1)
            ch['page_offset'] = ch.get('start', 0)
        return chunks
    for ch in chunks:
        st = int(ch.get('start', 0))
        for i, (pos, page_num) in enumerate(page_markers[:-1]):
            nxt = page_markers[i+1][0]
            if pos <= st < nxt:
                ch['page'] = page_num if page_num is not None else 1
                ch['page_offset'] = max(0, st - pos)
                break
        else:
            ch['page'] = 1
            ch['page_offset'] = st
    return chunks


def sliding_window_chunk(text: str, window_size: int = 2000, step_size: int = 1000) -> List[Dict[str, Any]]:
    chunks: List[Dict[str, Any]] = []
    n = len(text or '')
    start = 0
    cid = 1
    while start < n:
        chunk_text = text[start:start+window_size]
        if chunk_text.strip():
            chunks.append({
                'chunk_id': f'chunk-{cid}',
                'section': f'Window {cid}',
                'text': chunk_text.strip(),
                'start': start,
            })
        if start + window_size >= n:
            break
        start += step_size
        cid += 1
    return tag_chunks_with_page(chunks, text)


# --- Boilerplate cleaning ---

def remove_boilerplate_phrases(text: str, boilerplate_patterns: Optional[List[str]] = None) -> str:
    if boilerplate_patterns is None:
        boilerplate_patterns = [
            r"for official use only \(fouo\)",
            r'disclaimer:.*the information contained in this document may contain privileged and confidential information including patient information protected by federal and state privacy laws\.',
            r"electronically signed by",
            r"dictated by",
            r"this note was generated by",
            r"page \d+ of \d+",
            r"^date[:\s].*?$",
            r"^time[:\s].*?$",
            r"^signed[:\s].*?$",
        ]
    cleaned = text or ''
    for pat in boilerplate_patterns:
        cleaned = re.sub(pat, '', cleaned, flags=re.IGNORECASE | re.MULTILINE)
    return cleaned.strip()


def clean_chunks_remove_duplicates_and_boilerplate(chunks: List[Dict[str, Any]], boilerplate_patterns: Optional[List[str]] = None) -> List[Dict[str, Any]]:
    if boilerplate_patterns is None:
        boilerplate_patterns = [
            r"electronically signed by",
            r"dictated by",
            r"this note was generated by",
            r"page \d+ of \d+",
            r"^date[:\s]",
            r"^time[:\s]",
            r"^signed[:\s]",
            r"for official use only \(fouo\)",
            r'disclaimer:.*the information contained in this document may contain privileged and confidential information including patient information protected by federal and state privacy laws\.',
        ]
    seen: set[str] = set()
    out: List[Dict[str, Any]] = []
    for ch in chunks:
        txt = (ch.get('text') or '').strip()
        if not txt:
            continue
        low = txt.lower()
        if low in seen:
            continue
        if any(re.search(pat, low) for pat in boilerplate_patterns):
            continue
        out.append(ch)
        seen.add(low)
    return out


# --- BM25 ---

def build_bm25_index(chunks: List[Dict[str, Any]], use_bigrams: bool = True, k1: float = 1.5, b: float = 0.75) -> Dict[str, Any]:
    from collections import defaultdict
    postings = defaultdict(lambda: defaultdict(int))
    doc_len: List[int] = []
    for i, ch in enumerate(chunks):
        toks = _tokenize(ch.get('text', ''))
        if use_bigrams:
            toks = toks + _bigrams(toks)
        doc_len.append(len(toks))
        for t in toks:
            postings[t][i] += 1
    N = len(chunks) or 1
    df = {t: len(docs) for t, docs in postings.items()}
    avgdl = (sum(doc_len) / float(N)) if N else 0.0
    return {'postings': postings, 'df': df, 'N': N, 'doc_len': doc_len, 'avgdl': avgdl, 'k1': float(k1), 'b': float(b), 'use_bigrams': bool(use_bigrams)}


def bm25_score_query(query: str, bm25: Dict[str, Any]) -> Dict[int, float]:
    from collections import defaultdict
    if not bm25 or not query:
        return {}
    toks = _tokenize(query)
    if bm25.get('use_bigrams'):
        toks = toks + _bigrams(toks)
    N = bm25['N'] or 1
    avgdl = bm25['avgdl'] or 1.0
    k1 = bm25['k1']
    b = bm25['b']
    df = bm25['df']
    postings = bm25['postings']
    doc_len = bm25['doc_len']
    scores: Dict[int, float] = defaultdict(float)
    for t in set(toks):
        n_qi = df.get(t, 0)
        if n_qi == 0:
            continue
        idf = max(0.0, math.log(((N - n_qi + 0.5) / (n_qi + 0.5)) + 1e-9))
        for doc_id, tf in postings.get(t, {}).items():
            dl = doc_len[doc_id] or 1
            denom = tf + k1 * (1 - b + b * (dl / avgdl))
            scores[doc_id] += idf * ((tf * (k1 + 1)) / denom)
    return scores


# --- Hybrid search ---

def _parse_any_date(s: Optional[str]):
    if not s:
        return None
    s = str(s).strip()
    try:
        if s.endswith('Z'):
            return datetime.fromisoformat(s.replace('Z', '+00:00'))
        return datetime.fromisoformat(s)
    except Exception:
        for fmt in ('%Y-%m-%d', '%m/%d/%Y', '%m/%d/%y'):
            try:
                return datetime.strptime(s, fmt).replace(tzinfo=timezone.utc)
            except Exception:
                pass
    return None


def hybrid_search(query: str, chunks: List[Dict[str, Any]], vectors: Optional[np.ndarray], bm25_index: Optional[Dict[str, Any]] = None, top_k: int = 20, section_boosts: Optional[Dict[str, float]] = None) -> List[Dict[str, Any]]:
    if section_boosts is None:
        section_boosts = {
            'assessment_plan': 0.15,
            'history_present_illness': 0.12,
            'subjective': 0.10,
            'hospital_course': 0.08,
        }
    bm25_scores: Dict[int, float] = bm25_score_query(query, bm25_index) if bm25_index else {}

    # Semantic cosine
    cosine_by_chunk: Dict[int, float] = {}
    try:
        if vectors is not None and getattr(vectors, 'shape', (0,))[0] > 0:
            v = vectors.astype(np.float32)
            v = v / (np.linalg.norm(v, axis=1, keepdims=True) + 1e-8)
            qvec = np.array(emb_api.get_embeddings([query])[0], dtype=np.float32)
            qn = qvec / (np.linalg.norm(qvec) + 1e-8)
            sims = (v @ qn).tolist()
            # Take top-K*3 for semantic
            k = min(max(top_k * 3, top_k), len(sims))
            top_idx = np.argsort(-np.array(sims))[:k].tolist()
            for i in top_idx:
                cosine_by_chunk[i] = float(sims[i])
    except Exception:
        cosine_by_chunk = {}

    cand_ids = set(cosine_by_chunk.keys()) | set(bm25_scores.keys())
    if not cand_ids:
        return []

    def z_norm(d: Dict[int, float]) -> Dict[int, float]:
        if not d:
            return {}
        arr = np.array(list(d.values()), dtype=np.float32)
        mu, sigma = float(arr.mean()), float(arr.std() + 1e-6)
        return {k: (v - mu) / sigma for k, v in d.items()}

    cos_z = z_norm({i: float(cosine_by_chunk.get(i, 0.0)) for i in cand_ids})
    bm_z = z_norm({i: float(bm25_scores.get(i, 0.0)) for i in cand_ids}) if any(v > 0 for v in bm25_scores.values()) else {i: 0.0 for i in cand_ids}

    w_sem, w_kw = 0.65, 0.35
    w_rec = 0.08
    now = datetime.now(timezone.utc)
    combined: Dict[int, float] = {}
    q_toks = set(_tokenize(query))
    for i in cand_ids:
        if not (0 <= int(i) < len(chunks)):
            continue
        base = w_sem * cos_z.get(i, 0.0) + w_kw * bm_z.get(i, 0.0)
        boost = 0.0
        sec = (chunks[i].get('section') or '').lower()
        if sec in section_boosts:
            boost += section_boosts[sec]
        dt = _parse_any_date(chunks[i].get('date'))
        if dt:
            if not dt.tzinfo:
                dt = dt.replace(tzinfo=timezone.utc)
            age_days = max(0.0, (now - dt).total_seconds() / 86400.0)
            recency = math.exp(-age_days / 365.0)
            boost += w_rec * recency
        try:
            ttl = chunks[i].get('title') or ''
            if ttl:
                t_toks = set(_tokenize(ttl))
                if t_toks and (q_toks & t_toks):
                    boost += 0.12
        except Exception:
            pass
        # Optional per-chunk tag boost (precomputed by provider based on nationalTitle/localTitle)
        try:
            tb = float(chunks[i].get('tag_boost') or 0.0)
            boost += tb
        except Exception:
            pass
        combined[i] = base + boost

    ordered = sorted(combined.keys(), key=lambda x: combined[x], reverse=True)
    results: List[Dict[str, Any]] = []
    seen_note: Dict[str, int] = {}
    for i in ordered:
        if not (0 <= int(i) < len(chunks)):
            continue
        nid = str(chunks[i].get('note_id') or chunks[i].get('chunk_id') or i)
        if seen_note.get(nid, 0) >= 3:  # cap 3 excerpts per note
            continue
        results.append(chunks[i])
        seen_note[nid] = seen_note.get(nid, 0) + 1
        if len(results) >= top_k:
            break
    return results


# --- RagEngine: builds chunks and indices from VPR documents ---

class RagEngine:
    def __init__(self, window_size: int = 2000, step_size: int = 1000):
        self.window_size = window_size
        self.step_size = step_size
        self._chunks: List[Dict[str, Any]] = []
        self._bm25: Optional[Dict[str, Any]] = None
        self._vectors: Optional[np.ndarray] = None

    @staticmethod
    def _extract_note_text(it: Dict[str, Any]) -> str:
        # VPR documents often have a 'text' array where each element has 'content' or 'text'
        try:
            txt = it.get('text')
            if isinstance(txt, list) and txt:
                parts: List[str] = []
                for t in txt:
                    if isinstance(t, dict):
                        s = t.get('content') or t.get('text') or ''
                        if s:
                            parts.append(str(s))
                    elif isinstance(t, str):
                        parts.append(t)
                if parts:
                    return "\n".join(parts)
            if isinstance(txt, str):
                return txt
        except Exception:
            pass
        # Other fallbacks
        for key in ('content', 'document', 'body'):
            try:
                val = it.get(key)
                if isinstance(val, str) and val.strip():
                    return val
            except Exception:
                continue
        return ''

    def build_chunks_from_vpr_documents(self, vpr_payload: Any) -> List[Dict[str, Any]]:
        items = []
        try:
            if isinstance(vpr_payload, dict):
                d = vpr_payload.get('data') or {}
                items = (d.get('items') if isinstance(d, dict) else None) or vpr_payload.get('items') or []
            elif isinstance(vpr_payload, list):
                items = vpr_payload
        except Exception:
            items = []
        all_chunks: List[Dict[str, Any]] = []
        for it in items:
            if not isinstance(it, dict):
                continue
            text = self._extract_note_text(it)
            if not text:
                continue
            title = it.get('localTitle') or it.get('title') or it.get('displayName') or ''
            date = it.get('referenceDateTime') or it.get('dateTime') or it.get('entered') or None
            note_id = it.get('uid') or it.get('id') or it.get('uidLong') or None
            # Clean boilerplate inside full note once before chunking
            text_clean = remove_boilerplate_phrases(text)
            chunks = sliding_window_chunk(text_clean, self.window_size, self.step_size)
            for ch in chunks:
                ch['title'] = title
                ch['date'] = date
                ch['note_id'] = note_id
            all_chunks.extend(chunks)
        # drop empties/dups
        all_chunks = clean_chunks_remove_duplicates_and_boilerplate(all_chunks)
        self._chunks = all_chunks
        return all_chunks

    def index(self):
        texts = [c.get('text') or '' for c in self._chunks]
        # Embeddings
        try:
            vecs_list = emb_api.get_embeddings(texts)
            if vecs_list:
                self._vectors = np.array(vecs_list, dtype=np.float32)
            else:
                self._vectors = None
        except Exception:
            self._vectors = None
        # BM25
        self._bm25 = build_bm25_index(self._chunks)

    def retrieve(self, query: str, top_k: int = 12) -> List[Dict[str, Any]]:
        return hybrid_search(query, self._chunks, self._vectors, bm25_index=self._bm25, top_k=top_k)

    @property
    def chunks(self) -> List[Dict[str, Any]]:
        return self._chunks
 